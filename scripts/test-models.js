/**
 * Script para testar os modelos baixados
 * Execute: node scripts/test-models.js
 */

const { pipeline } = require("@xenova/transformers")
const path = require("path")

const CACHE_DIR = path.join(__dirname, "..", "models", "huggingface")
process.env.TRANSFORMERS_CACHE = CACHE_DIR

async function testWhisper() {
    console.log("\nüé§ Testando Whisper (transcri√ß√£o de √°udio)...")

    try {
        const transcriber = await pipeline(
            "automatic-speech-recognition",
            "Xenova/whisper-tiny",
            { cache_dir: CACHE_DIR },
        )

        // Whisper precisa de arquivo de √°udio real, ent√£o apenas carregamos o modelo
        console.log("   ‚úÖ Modelo Whisper carregado com sucesso!")
        console.log("   ‚ÑπÔ∏è  Para testar transcri√ß√£o, forne√ßa um arquivo de √°udio")

        return true
    } catch (error) {
        console.error("   ‚ùå Erro:", error.message)
        return false
    }
}

async function testTextEmbedding() {
    console.log("\nüìù Testando Text Embedding (all-MiniLM-L6-v2)...")

    try {
        const extractor = await pipeline("feature-extraction", "Xenova/all-MiniLM-L6-v2", {
            cache_dir: CACHE_DIR,
        })

        const text = "Este √© um texto de teste para gerar embeddings."
        const output = await extractor(text, { pooling: "mean", normalize: true })

        console.log(`   ‚úÖ Embedding gerado com sucesso!`)
        console.log(`   üìê Dimens√£o: ${output.data.length}`)
        console.log(`   üî¢ Primeiros valores: [${output.data.slice(0, 5).map((v) => v.toFixed(4)).join(", ")}...]`)

        return true
    } catch (error) {
        console.error("   ‚ùå Erro:", error.message)
        return false
    }
}

async function testCLIP() {
    console.log("\nüñºÔ∏è  Testando CLIP (embeddings visuais)...")

    try {
        const classifier = await pipeline(
            "zero-shot-image-classification",
            "Xenova/clip-vit-base-patch32",
            { cache_dir: CACHE_DIR },
        )

        // CLIP precisa de imagem real, ent√£o apenas carregamos o modelo
        console.log("   ‚úÖ Modelo CLIP carregado com sucesso!")
        console.log("   ‚ÑπÔ∏è  Para testar embeddings visuais, forne√ßa uma imagem")

        return true
    } catch (error) {
        console.error("   ‚ùå Erro:", error.message)
        return false
    }
}

async function main() {
    console.log("üß™ Testando modelos do Hugging Face...\n")

    const results = []

    results.push(await testWhisper())
    results.push(await testTextEmbedding())
    results.push(await testCLIP())

    const successCount = results.filter((r) => r).length

    console.log("\n" + "=".repeat(60))
    console.log("üìä RESULTADO DOS TESTES")
    console.log("=".repeat(60))
    console.log(`‚úÖ ${successCount}/3 modelos funcionando corretamente`)

    if (successCount === 3) {
        console.log("\nüéâ Todos os modelos est√£o prontos para uso!")
    } else {
        console.log("\n‚ö†Ô∏è  Alguns modelos falharam. Verifique os erros acima.")
    }
}

main().catch((error) => {
    console.error("\n‚ùå Erro fatal:", error)
    process.exit(1)
})

